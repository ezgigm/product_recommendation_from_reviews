{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe and series \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn imports for modeling part\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# To plot\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline    \n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My aim is to compare products and determine less seller products with giving importance to negative reviewed books to take action. So, to focus on less ratings, I will divide my target to two-class as positive and negative where 1 and 2 rating values counted as negative and others are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_two_sentiment(overall):\n",
    "    '''This function encodes the rating 1 and 2 as 0, others as 1'''\n",
    "    if overall >= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['overall'].apply(calc_two_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2031419\n",
       "0     109546\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_torch = df.head(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write codes easily and keep less data in memory, I will just only choose the columns which I need for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_torch= df_torch.loc[:, ['review_clean', 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use more easily I will divide train-test splits and write them csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df_torch, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 80000\n",
      "Number of testing examples: 20000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_torch['review_clean']\n",
    "# y = df_torch['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data to Torch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the important concepts of TorchText is the Field function, which defines how the data should be processed. \n",
    "\n",
    "I will use TEXT to define how the reviews will be processed and use LABEL field to process the target. As a preprocessing technique, I will use bi-grams. It creates a set of co-occuring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(text):\n",
    "    '''creating set of co-occuring words'''\n",
    "    bi_grams = set(zip(*[text[i:] for i in range(2)]))\n",
    "    for bi_gram in bi_grams:\n",
    "        text.append(' '.join(bi_gram))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'this', 'book', 'I love', 'this book', 'love this']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check bi-gram function is working proporly or not\n",
    "generate_bigrams(['I', 'love', 'this', 'book'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My bi-gram function is working properly, I can see two-words couples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will define my model to preprocess with bi-grams, SpaCy tokenizer and LabelField to handle the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', preprocessing = generate_bigrams)\n",
    "TARGET = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 1234\n",
    "\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# TEXT = data.Field(tokenize = 'spacy')\n",
    "# LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_train = [('review_clean', TEXT),('sentiment', TARGET)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With using TabularDataset, we will take our train, test splits easily each time and preprocessed with bi-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking training data from train.csv\n",
    "train_data = data.TabularDataset(path = 'train.csv',\n",
    "                                 format = 'csv',\n",
    "                                 fields = fields_train,\n",
    "                                 skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking test data from test.cvs\n",
    "test_data = data.TabularDataset(path = 'test.csv',\n",
    "                                 format = 'csv',\n",
    "                                 fields = fields_train,\n",
    "                                 skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To check the first elements in train and test\n",
    "# print(vars(train_data[0]))\n",
    "# print(vars(test_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to split a validation data from my train data, to make sure my model is doing good. I will use default for split sizes and define my random seed to get same data each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating validation set from train data\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 56000\n",
      "Number of validation examples: 24000\n",
      "Number of testing examples: 20000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I need to build a vocabulary. There are lots of words so I will define maximum top words sizes. Then, I will load the pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary with Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only build vocabulary on train set. Because, in machine learning models test set must not be seen before to test it well. I do not add validation set also, because I want it to reflect the test set as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique tokens in TEXT vocabulary: 25002\n",
      "# of unique tokens in TARGET vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"# of unique tokens in TARGET vocabulary: {len(TARGET.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose my max vocabulary size 25000, it means there is two additional tokens like <...> default. Because all sentences in the batches must be at same size. To make each sentence equal in the batch, it padded longer or shorter batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 239106), ('and', 150173), ('a', 139894), ('i', 135734), ('to', 130913), ('of', 98771), (' ', 88659), ('is', 78752), ('this', 76517), ('it', 74223), ('in', 65221), ('was', 60491), ('that', 56178), ('book', 52295), ('for', 44131), ('story', 38653), ('but', 37886), ('her', 37795), ('with', 37778), ('read', 35331), ('you', 34217), ('nt', 33620), ('\\n\\n', 32602), ('she', 28880), ('not', 28241)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(25)) # to see most common words in the vocabulary with their frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I create my vocabulary using pre-trained embeddings. The final step of preparing data to Torch model is creating iterators. I will iterate train and evaluation loop and get a batch of examples which indexed and converted into tensors for each iteration. I will use Iterator function of torch. Also, I need to keep the tensors which returned by iterators in GPU so I will use torch.device function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set batch size and iterators for train and validation data \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator = data.Iterator(dataset = train_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = True, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)\n",
    "valid_iterator = data.Iterator(dataset = valid_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = False, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = data.Iterator(dataset = test_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = False, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ready classes to building a model. I prefer to use FastText class for baseline model, because gets comparable results significantly faster and using around half of the parameters. The details about this class can be found in [Bag of Tricks for Efficient Text Classification paper](https://arxiv.org/abs/1607.01759). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        \n",
    "        #pooled = [batch size, embedding_dim]\n",
    "                \n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model only has 2 layers that have any parameters, the linear and the embedding layer. There in no RNN layer. It will calculate the word embedding by using embedding layer, and taking average of them feeds the linear layer. Now, I will create my FastText class with defining dimensions and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab) #vocabulary size \n",
    "EMBEDDING_DIM = 100 # embedding dimension\n",
    "OUTPUT_DIM = 1 # our output has only 2 classes - 0/1. So, it is one-dimensional.\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # string to integer method on padding tokens\n",
    "\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare trainable parameters in different models, count parameters function will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,500,301 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will copy pre-trained vectors to my embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding:\n",
    "    __author__ = \"Edward Ma\"\n",
    "    __copyright__ = \"Copyright 2018, Edward Ma\"\n",
    "    __credits__ = [\"Edward Ma\"]\n",
    "    __license__ = \"Apache\"\n",
    "    __version__ = \"2.0\"\n",
    "    __maintainer__ = \"Edward Ma\"\n",
    "    __email__ = \"makcedward@gmail.com\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.model = {}\n",
    "        \n",
    "    def convert(self, source, ipnut_file_path, output_file_path):\n",
    "        if source == 'glove':\n",
    "            input_file = datapath(ipnut_file_path)\n",
    "            output_file = get_tmpfile(output_file_path)\n",
    "            glove2word2vec(input_file, output_file)\n",
    "        elif source == 'word2vec':\n",
    "            pass\n",
    "        elif source == 'fasttext':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "        \n",
    "    def load(self, source, file_path):\n",
    "        print(datetime.datetime.now(), 'start: loading', source)\n",
    "        if source == 'glove':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "        elif source == 'word2vec':\n",
    "            self.model[source] = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "        elif source == 'fasttext':\n",
    "            self.model[source] = gensim.models.wrappers.FastText.load_fasttext_format(file_path)\n",
    "        else:\n",
    "            raise ValueError('Possible value of source are glove, word2vec, fasttext')\n",
    "            \n",
    "        print(datetime.datetime.now(), 'end: loading', source)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e73428c613a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword2vec_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model.embedding.weight.data.copy_(pretrained_embeddings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2vec_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "word_embedding = WordEmbedding()\n",
    "word_embedding.load(source='word2vec', file_path=word2vec_file_path)\n",
    "\n",
    "# model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 1.1952, -1.7452, -1.4624,  ..., -0.6250, -0.0424, -0.2210],\n",
       "        [-0.8913, -1.5787, -1.2862,  ..., -0.6323,  0.9110,  0.9361],\n",
       "        [-2.0606, -0.3205, -0.4881,  ...,  0.1854,  0.3752, -0.1491]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must assign zero for initial weight for unknown and padding tokens. I have already defined padding token before as PAD_IDX. So, I will define unknows as UNK_IDX and set initials to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, firstly I will create optimizer and criterion. Optimizer updates parameters of module. I will use SGD and Adam as optimizer. SGD is a variant of gradient descent. It does not perform on whole dataset, it computes on a small subset or random selection. It performs good when the learning rate is low. Optimizer needs two parameters, one is optimizer type and second is learning rate. Adam optimizer is a technique which implementing adaptive learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried both optimizers one by one with uncommenting the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will define loss function. My target contains binary labels, so I will choose binary loss function as criterion.\n",
    "Cross-entropy loss is commonly used for classification porblems. Also, BCEWithLogitsLoss is contains one sigmoid layer and binary cross-entropy loss. So, I will use this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# keeping model and criterion in GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss will be calculated by using criterion but I want to see accuracy to compare models. This function turn the values to 0-1 with rounding them in sigmoid layer. Then, it calculates the rounded predictions equal actual labels and take the mean of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(pred, target):\n",
    "      \n",
    "    # rounding predictions to the closest integer\n",
    "    rounded_pred = torch.round(torch.sigmoid(pred))\n",
    "    true = (rounded_pred == target).float() # convert into float for taking mean \n",
    "    accuracy = true.sum() / len(true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the train method\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0 # \n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator: # for each batch\n",
    "        \n",
    "        optimizer.zero_grad() # zero gradient\n",
    "        # PyTorch does not automatically zero the gradients calculated from the last gradient calculation\n",
    "        \n",
    "        predictions = model(batch.review_clean).squeeze(1) # with feeding batch with reviews no need to .forward\n",
    "        \n",
    "        '''squeeze for removing dimension in the list and taking only batch size bec. torch wants\n",
    "        predictions input as batch size'''\n",
    "        \n",
    "        loss = criterion(predictions, batch.sentiment) # calculating loss\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.sentiment) # calculating accuracy with taking mean\n",
    "        \n",
    "        loss.backward() #gradient of each parameter\n",
    "        \n",
    "        optimizer.step() #update the optimizer algorithm\n",
    "        \n",
    "        # loss and accuracy by epoches\n",
    "        epoch_loss += loss.item() \n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) # returning loss and acc avg across epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do same function for evaluate validation part below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    '''Evaluating validation set'''\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.review_clean).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.sentiment)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.sentiment)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also use a function which informs that how long each epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model for Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 39s\n",
      "\tTrain Loss: 0.431 | Train Acc: 91.61%\n",
      "\t Val. Loss: 0.748 |  Val. Acc: 91.67%\n",
      "Epoch: 02 | Epoch Time: 2m 24s\n",
      "\tTrain Loss: 0.313 | Train Acc: 91.61%\n",
      "\t Val. Loss: 0.422 |  Val. Acc: 91.60%\n",
      "Epoch: 03 | Epoch Time: 2m 11s\n",
      "\tTrain Loss: 0.235 | Train Acc: 91.80%\n",
      "\t Val. Loss: 0.449 |  Val. Acc: 92.43%\n",
      "Epoch: 04 | Epoch Time: 2m 31s\n",
      "\tTrain Loss: 0.192 | Train Acc: 92.48%\n",
      "\t Val. Loss: 0.597 |  Val. Acc: 92.25%\n",
      "Epoch: 05 | Epoch Time: 2m 28s\n",
      "\tTrain Loss: 0.171 | Train Acc: 93.15%\n",
      "\t Val. Loss: 0.739 |  Val. Acc: 92.18%\n"
     ]
    }
   ],
   "source": [
    "# with Adam optimizer\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.408 | Test Acc: 91.81%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 17s\n",
      "\tTrain Loss: 0.652 | Train Acc: 73.05%\n",
      "\t Val. Loss: 0.526 |  Val. Acc: 91.41%\n",
      "Epoch: 02 | Epoch Time: 2m 32s\n",
      "\tTrain Loss: 0.535 | Train Acc: 91.63%\n",
      "\t Val. Loss: 0.415 |  Val. Acc: 91.42%\n",
      "Epoch: 03 | Epoch Time: 2m 14s\n",
      "\tTrain Loss: 0.462 | Train Acc: 91.63%\n",
      "\t Val. Loss: 0.357 |  Val. Acc: 91.44%\n",
      "Epoch: 04 | Epoch Time: 2m 7s\n",
      "\tTrain Loss: 0.414 | Train Acc: 91.63%\n",
      "\t Val. Loss: 0.326 |  Val. Acc: 91.44%\n",
      "Epoch: 05 | Epoch Time: 2m 7s\n",
      "\tTrain Loss: 0.382 | Train Acc: 91.63%\n",
      "\t Val. Loss: 0.309 |  Val. Acc: 91.44%\n"
     ]
    }
   ],
   "source": [
    "# with SGD optimizer\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.299 | Test Acc: 91.96%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Tri-Gram Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigrams(text):\n",
    "    '''creating set of 3 co-occuring words'''\n",
    "    tri_grams = set(zip(*[text[i:] for i in range(3)]))\n",
    "    for tri_gram in tri_grams:\n",
    "        text.append(' '.join(tri_gram))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'this', 'book', 'I love this', 'love this book']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check tri-gram function is working proporly or not\n",
    "generate_trigrams(['I', 'love', 'this', 'book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', preprocessing = generate_trigrams)\n",
    "TARGET = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_train = [('review_clean', TEXT),('sentiment', TARGET)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking training data from train.csv\n",
    "train_data = data.TabularDataset(path = 'train.csv',\n",
    "                                 format = 'csv',\n",
    "                                 fields = fields_train,\n",
    "                                 skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking test data from test.cvs\n",
    "test_data = data.TabularDataset(path = 'test.csv',\n",
    "                                 format = 'csv',\n",
    "                                 fields = fields_train,\n",
    "                                 skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vars(train_data[0])) # to check tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating validation set from train data\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "TARGET.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator = data.Iterator(dataset = train_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = True, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)\n",
    "valid_iterator = data.Iterator(dataset = valid_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = False, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = data.Iterator(dataset = test_data, batch_size = BATCH_SIZE,device = device, \n",
    "                               shuffle = None, train = False, sort_key = lambda x: len(x.review_clean), \n",
    "                               sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab) #vocabulary size \n",
    "EMBEDDING_DIM = 100 # embedding dimension\n",
    "OUTPUT_DIM = 1 # our output has only 2 classes - 0/1. So, it is one-dimensional.\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # string to integer method on padding tokens\n",
    "\n",
    "model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.6060, -0.3566,  0.7984,  ..., -0.5360, -1.6375, -0.1762],\n",
       "        [-0.0916, -1.7693,  0.2442,  ..., -1.3095, -0.1623, -2.5278],\n",
       "        [ 0.4193, -1.6199, -0.2880,  ...,  1.2291,  0.9729,  0.9876]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# keeping model and criterion in GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 48s\n",
      "\tTrain Loss: 0.452 | Train Acc: 90.65%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 91.75%\n",
      "Epoch: 02 | Epoch Time: 2m 40s\n",
      "\tTrain Loss: 0.327 | Train Acc: 91.64%\n",
      "\t Val. Loss: 0.431 |  Val. Acc: 91.88%\n",
      "Epoch: 03 | Epoch Time: 2m 44s\n",
      "\tTrain Loss: 0.249 | Train Acc: 91.74%\n",
      "\t Val. Loss: 0.411 |  Val. Acc: 92.38%\n",
      "Epoch: 04 | Epoch Time: 2m 36s\n",
      "\tTrain Loss: 0.206 | Train Acc: 92.14%\n",
      "\t Val. Loss: 0.556 |  Val. Acc: 92.13%\n",
      "Epoch: 05 | Epoch Time: 2m 28s\n",
      "\tTrain Loss: 0.183 | Train Acc: 92.63%\n",
      "\t Val. Loss: 0.685 |  Val. Acc: 92.33%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut3-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.418 | Test Acc: 92.06%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut3-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 2s\n",
      "\tTrain Loss: 0.593 | Train Acc: 91.67%\n",
      "\t Val. Loss: 0.505 |  Val. Acc: 91.73%\n",
      "Epoch: 02 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.498 | Train Acc: 91.67%\n",
      "\t Val. Loss: 0.409 |  Val. Acc: 91.73%\n",
      "Epoch: 03 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.438 | Train Acc: 91.67%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 91.73%\n",
      "Epoch: 04 | Epoch Time: 2m 2s\n",
      "\tTrain Loss: 0.398 | Train Acc: 91.67%\n",
      "\t Val. Loss: 0.325 |  Val. Acc: 91.73%\n",
      "Epoch: 05 | Epoch Time: 2m 0s\n",
      "\tTrain Loss: 0.371 | Train Acc: 91.67%\n",
      "\t Val. Loss: 0.308 |  Val. Acc: 91.73%\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # to keep model for test set\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.312 | Test Acc: 91.48%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not forget resources\n",
    "\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis\n",
    "\n",
    "https://www.kaggle.com/lalwaniabhishek/abhishek-lalwani-bits-twitter-text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
